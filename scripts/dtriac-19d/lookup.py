"""lookup.py

Look up technologies.

Usage:

$ python lookup.py --compile-technologies CLASSIFIER_DIRECTORY
$ python lookup.py --expand-technologies
$ python lookup.py -d DATA_DIR -f FILELIST -s START -e END


Uses two data files from the Brandeis Technology Finder and one from the
grounding component:

- data/technologies/technologies.txt
- data/technologies/technologies-annotated.txt
- data/wiki/wiki-titles-uniq-nr.txt

The first is created by compile_technologies(), which runs on the output of the
Techknowledgist classifier code. The second was at some point created from the
first by selecting the 2000+ most frequent technologies listed on descending
frequency (those occurring 10 times or more) and then manually annotating the
lines that are not technology terms. As of Jan 7th 2020 only the first 300 lines
were annotated. The third file has all the wiki titles generated by the grounding
component.

"""

import os
import sys
from collections import Counter

from lif import Container, LIF, View, Annotation
from utils import get_options, process_list, ensure_directory, create_view


DEBUG = False

TECHNOLOGY_LIST = 'data/technologies/technologies.txt'
TECHNOLOGY_ANNOTATIONS = 'data/technologies/technologies-annotated.txt'
TECHNOLOGY_HEADS = 'data/technologies/technologies-heads.txt'
WIKI_TITLES = 'data/wiki/wiki-titles-uniq-nr.txt'
WIKI_TITLES_ANNOTATIONS = 'data/wiki/wiki-titles-uniq-nr-anno.txt'

TECHNOLOGIES = None


# this value was calculated with longest_technology().
LONGEST_TECHNOLOGY = 7

# file with scores in the classification directory
SCORES_FILE = 'classify.MaxEnt.out.s4.scores.sum.az'

MIN_SCORE = 0.5
MIN_COUNT = 5

# only use wiki titles if they are associated with at least this number of
# documents
MIN_WIKI_TITLE_COUNT = 10


if DEBUG:
    OUT = open('lists/list-technologies-found.txt', 'w')


def compile_technologies(classification):
    """Create a list of technologies from the output of the technology
    classifier. Include only the terms with a score of at least MIN_SCORE and a
    frequency of at least MIN_COUNT. Save tab-separated tuples with count, score
    and term."""
    classification_file = os.path.join(classification, SCORES_FILE)
    with open(classification_file) as fh_in, \
         open(TECHNOLOGY_LIST, 'w') as fh_out:
        for line in fh_in:
            if line[0] in ' \t\n':
                continue
            term, score, count, low, high = line.strip().split('\t')
            score = float(score)
            count = int(count)
            if score >= MIN_SCORE and count >= MIN_COUNT:
                fh_out.write("%d\t%f\t%s\n" % (count, score, term))


def expand_technologies():
    _create_technology_heads_file()


def _create_technology_heads_file():
    # created so we can annotate it for technology terms
    heads = {}
    for line in open(TECHNOLOGY_LIST):
        count, score, term = line.strip().split('\t')
        count = int(count)
        term_tokens =  term.split()
        head = term_tokens[-1]
        heads[head] = heads.get(head, 0) + count
        if len(term_tokens) > 3:
            head = '%s %s' % (term_tokens[-2], term_tokens[-1])
            heads[head] = heads.get(head, 0) + count
    heads = Counter(heads)
    with open(TECHNOLOGY_HEADS, 'w') as fh:
        for term, count in heads.most_common():
            fh.write("\t%s\t%s\n" % (count, term))


def _technologies_from_wiki_terms_file():
    """Return the technologies from the wiki titles annotation file."""
    technologies = set()
    for line in open(WIKI_TITLES_ANNOTATIONS):
        if line.startswith('#'):
            continue
        anno, count_plus_term = line.rstrip().split('\t')
        if anno:
            count, term = count_plus_term.strip().split(' ', 1)
            term_tokens = [t.strip('()') for t in term.split()]
            #print("%s\t%s\t%s\t%s" % (anno, count,term, term_tokens))
            if anno == '+':
                #print(anno, '\t', term_tokens)
                tterm = ' '.join(term_tokens)
                technologies.add(tterm)
            elif '-' in anno:
                start, end = anno.split('-')
                start = int(start) - 1
                end = int(end)
                #print(anno, '\t', term_tokens[start:end])
                tterm = ' '.join(term_tokens[start:end])
                technologies.add(tterm)
            else:
                start = int(anno) - 1
                end = start + 1
                #print(anno, '\t', term_tokens[start:end])
                tterm = ' '.join(term_tokens[start:end])
                technologies.add(tterm)
    return technologies


def lookup_technologies(data_dir, fname):
    subdir = os.path.split(fname)[0]
    pos_file = os.path.join(data_dir, 'pos', subdir, "%s.pos.lif" % subdir)
    tex_file = os.path.join(data_dir, 'tex', subdir, "%s.lup.lif" % subdir)
    ensure_directory(tex_file)
    lif = Container(pos_file).payload
    lif_tex = LIF(json_object=lif.as_json())
    pos_view = lif.get_view('v2')
    tex_view = create_view('tex', 'Technology', 'dtriac-pipeline:lookup.py')
    lif_tex.views = [tex_view]
    tokens = [a for a in pos_view.annotations if a.type.endswith('Token')]
    _lookup_technologies_in_tokens(lif, tokens, tex_view)
    lif_tex.write(fname=tex_file, pretty=True)


def _lookup_technologies_in_tokens(lif, tokens, tex_view):
    TECHNOLOGIES.reset_next_id()
    for i in range(len(tokens)):
        pairs = [(_get_text_from_tokens(tokens, i, i + j), j) for j in range(2,8)]
        for w, length in pairs:
            if w in TECHNOLOGIES.terms:
                anno = _create_annotation(lif, tokens, w, i, length, "technology")
                tex_view.annotations.append(anno)
            elif w in TECHNOLOGIES.wiki_terms:
                anno = _create_annotation(lif, tokens, w, i, length, "wiki_term")
                tex_view.annotations.append(anno)


def _get_text_from_tokens(tokens, p1, p2):
    return ' '.join([t.features.get('word') for t in tokens[p1:p2]])


def _create_annotation(lif, tokens, w, i, length, ttype):
    p1, p2, w_in_text = _get_match_information(lif, tokens, i, length)
    if DEBUG:
        OUT.write("%s\t%s\t%s\n" % (p1, p2, w))
    next_id = TECHNOLOGIES.get_next_id()
    json_obj = { "id": "t%d" % next_id,
                 "@type": 'http://vocab.lappsgrid.org/Technology',
                 "start": p1, "end": p2,
                 "features": { "term": w, "type": ttype }}
    return Annotation(json_obj)


def _get_match_information(lif, tokens, i, length):
    p1 = tokens[i].start
    p2 = tokens[i+length-1].end
    w_in_text = lif.text.value[p1:p2].replace('\n', '<EOL>')
    # print('   ', w, '--', w_in_text)
    return p1, p2, w_in_text


def longest_technology():
    lengths = [len(t.split()) for t in TECHNOLOGIES.terms]
    c = Counter(lengths)
    print(c)
    longest = 0
    for t in TECHNOLOGIES.terms:
        tokens = t.split()
        longest = max(longest, len(tokens))
    return longest


class TechnologyOntology(object):

    """TechnologyOntology is rather a big word for this since all this does at the
    moment is to keep a list of technologies and a stoplist of terms that are not
    technologies."""

    def __init__(self):
        self.next_id = 0
        self.terms = set()
        self.stoplist = set()
        self.wiki_terms = _technologies_from_wiki_terms_file()
        for line in open(TECHNOLOGY_ANNOTATIONS):
            if line.startswith('- '):
                self.stoplist.add(line.strip()[2:])
        for line in open(TECHNOLOGY_LIST):
            count, score, term = line.strip().split('\t')
            if not self.filter(term) and not term in self.stoplist:
                self.terms.add(term)

    def __len__(self):
        return len(self.terms)

    def __str__(self):
        return "<TechnologyOntology terms=%d wiki_terms=%d>" \
            % (len(self.terms), len(self.wiki_terms))

    def get_next_id(self):
        self.next_id += 1
        return self.next_id

    def reset_next_id(self):
        self.next_id += 0

    def filter(self, term):
        if (term[0] in '!%-\\©®°'
            or term.startswith('appendices')
            or term.startswith('figures')
            or term.startswith('c/o')
            or term.startswith('cents')
            or term.startswith('c0 ')
            or term.startswith('d ')
            or term.startswith('d. ')
            or len(term) > 50):
            return True
        return False


if __name__ == '__main__':

    if sys.argv[1] == "--compile-technologies":
        classification = sys.argv[2]
        compile_technologies(classification)
    elif sys.argv[1] == "--expand-technologies":
        expand_technologies()
    else:
        data_dir, filelist, start, end, crash = get_options()
        TECHNOLOGIES = TechnologyOntology()
        print(TECHNOLOGIES)
        # print("Loaded %s" % TECHNOLOGIES)
        # print(longest_technology())
        process_list(data_dir, filelist, start, end, crash, lookup_technologies)
